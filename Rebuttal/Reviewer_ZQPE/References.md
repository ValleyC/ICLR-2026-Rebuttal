# Unified References for Reviewer ZQPE Responses

This is a unified reference list for all responses to Reviewer ZQPE's comments.

---

## Core Diffusion-Based Methods

**[1] DIFUSCO**
Sun, Z., & Yang, Y. (2023). DIFUSCO: Graph-based diffusion solvers for combinatorial optimization. *Advances in Neural Information Processing Systems*, 36.

**[2] T2T**
Li, Y., Guo, J., Wang, R., & Yan, J. (2023). T2T: From distribution learning in training to gradient search in testing for combinatorial optimization. *Advances in Neural Information Processing Systems*, 36.

**[3] Fast-T2T**
Li, Y., Guo, J., Wang, R., Zha, H., & Yan, J. (2024). Fast T2T: Optimization consistency speeds up diffusion-based training-to-testing solving for combinatorial optimization. *Advances in Neural Information Processing Systems*, 37.

**[4] COExpander**
Ma, J., Pan, W., Li, Y., & Yan, J. (2025). COExpander: Adaptive solution expansion for combinatorial optimization. *Proceedings of the 42nd International Conference on Machine Learning*.

**[5] DiffUCO**
Sanokowski, S., Hochreiter, S., & Lehner, S. (2024). A diffusion model framework for unsupervised neural combinatorial optimization. *International Conference on Machine Learning*.

---

## Sampling-Based and Variational Methods

**[6] iSCO**
Sun, H., Goshvadi, K., Nova, A., Schuurmans, D., & Dai, H. (2023). Revisiting sampling for combinatorial optimization. *Proceedings of the 40th International Conference on Machine Learning*, 202, 32805-32824.

**[7] RLSA**
Feng, S., & Yang, Y. (2025). Regularized Langevin dynamics for combinatorial optimization. *Forty-second International Conference on Machine Learning*.

**[8] VAG-CO**
Sanokowski, S., Berghammer, W., Hochreiter, S., & Lehner, S. (2023). Variational annealing on graphs for combinatorial optimization. *Advances in Neural Information Processing Systems*, 36.

**[9] GFlowNets**
Zhang, D., Dai, H., Malkin, N., Courville, A., Bengio, Y., & Pan, L. (2023). Let the flows tell: Solving graph combinatorial optimization problems with GFlowNets. *Advances in Neural Information Processing Systems*, 36.

---

## RL-Based Methods

**[10] AM (Attention Model)**
Kool, W., van Hoof, H., & Welling, M. (2019). Attention, learn to solve routing problems! *International Conference on Learning Representations*.

**[11] POMO**
Kwon, Y.-D., Choo, J., Kim, B., Yoon, I., Gwon, Y., & Min, S. (2020). POMO: Policy optimization with multiple optima for reinforcement learning. *Advances in Neural Information Processing Systems*, 33.

**[12] Sym-NCO**
Kim, M., Park, J., Park, J., & Kwon, Y.-D. (2022). Sym-NCO: Leveraging symmetricity for neural combinatorial optimization. *Advances in Neural Information Processing Systems*, 35.

**[13] BQ-NCO**
Drakulic, D., Michel, S., Mai, F., Sors, A., & Andreoli, J.-M. (2023). BQ-NCO: Bisimulation quotienting for generalizable neural combinatorial optimization. *Advances in Neural Information Processing Systems*, 36.

**[14] PO (Preference Optimization)**
Pan, M., Lin, G., Luo, Y.-W., Zhu, B., Dai, Z., Sun, L., & Yuan, C. (2025). Preference optimization for combinatorial optimization problems. *Forty-second International Conference on Machine Learning*.

**[15] BOPO**
Liao, Z., Chen, J., Wang, D., Zhang, Z., & Wang, J. (2025). BOPO: Neural combinatorial optimization via best-anchored and objective-guided preference optimization. *arXiv preprint arXiv:2503.07580*.

---

## Unsupervised Methods

**[16] UTSP**
Min, Y., Bai, Y., & Gomes, C. P. (2023). Unsupervised learning for solving the travelling salesman problem. *Advances in Neural Information Processing Systems*, 36.

---

## Generalist and Problem Reduction Approaches

**[17] UniCO**
Pan, W., Xiong, H., Ma, J., Zhao, W., Li, Y., & Yan, J. (2025). UniCO: On unified combinatorial optimization via problem reduction to matrix-encoded general TSP. *The Thirteenth International Conference on Learning Representations*.

**[18] GOAL**
Drakulic, D., Michel, S., & Andreoli, J.-M. (2025). GOAL: A generalist combinatorial optimization agent learner. *The Thirteenth International Conference on Learning Representations*.

---

## Classical and Hybrid Methods

**[19] HGS (Hybrid Genetic Search)**
Vidal, T., Crainic, T. G., Gendreau, M., & Prins, C. (2013). A hybrid genetic algorithm for multidepot and periodic vehicle routing problems. *Operations Research*, 60(3), 611-624.

**[20] NeuroLKH**
Xin, L., Song, W., Cao, Z., & Zhang, J. (2021). NeuroLKH: Combining deep learning model with Lin-Kernighan-Helsgaun heuristic for solving the traveling salesman problem. *Advances in Neural Information Processing Systems*, 34, 7472-7483.

**[21] GLOP**
Ye, H., Wang, J., Cao, Z., Zhang, G., & Song, L. (2024). GLOP: Learning global partition and local construction for solving large-scale routing problems in real-time. *AAAI Conference on Artificial Intelligence*.

---

## ATSP-Specific Methods

**[22] MatNet**
Kwon, Y.-D., Choo, J., Yoon, I., Park, M., Park, D., & Gwon, Y. (2021). Matrix encoding networks for neural combinatorial optimization. *Advances in Neural Information Processing Systems*, 34, 5138-5149.

**[23] GREAT**
Kuhn, T., Lutzeyer, J. F., Walther, E., & Schubert, M. (2024). A GREAT architecture for edge-based graph problems like TSP. *arXiv preprint arXiv:2408.16717*.

---

## Other Neural Methods

**[24] Pointer Networks**
Vinyals, O., Fortunato, M., & Jaitly, N. (2015). Pointer networks. *Advances in Neural Information Processing Systems*, 28.

---

## Theoretical Foundations

**[25] DDPM (Denoising Diffusion Probabilistic Models)**
Ho, J., Jain, A., & Abbeel, P. (2020). Denoising diffusion probabilistic models. *Advances in Neural Information Processing Systems*, 33, 6840-6851.

**[26] Score-Based Generative Modeling**
Song, Y., & Ermon, S. (2019). Generative modeling by estimating gradients of the data distribution. *Advances in Neural Information Processing Systems*, 32.

**[27] Classifier-Free Guidance**
Ho, J., & Salimans, T. (2022). Classifier-free diffusion guidance. *arXiv preprint arXiv:2207.12598*.

**[28] DDIM (Denoising Diffusion Implicit Models)**
Song, J., Meng, C., & Ermon, S. (2021). Denoising diffusion implicit models. *International Conference on Learning Representations*.

**[29] Learning Rate Schedules**
Loshchilov, I., & Hutter, F. (2017). SGDR: Stochastic gradient descent with warm restarts. *International Conference on Learning Representations*.

**[30] Finsler Multi-Dimensional Scaling**
Dag√®s, T., Weber, S., Lin, Y.-W. E., Talmon, R., Cremers, D., Lindenbaum, M., Bruckstein, A. M., & Kimmel, R. (2025). Finsler multi-dimensional scaling: Manifold learning for asymmetric dimensionality reduction and embedding. *Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition*.

**[31] Score-Based Generative Modeling through SDEs**
Song, Y., Sohl-Dickstein, J., Kingma, D. P., Kumar, A., Ermon, S., & Poole, B. (2021). Score-based generative modeling through stochastic differential equations. *International Conference on Learning Representations*.

**[32] Improved Denoising Diffusion Probabilistic Models**
Nichol, A. Q., & Dhariwal, P. (2021). Improved denoising diffusion probabilistic models. *International Conference on Machine Learning*, 139, 8162-8171.

---

## Reference Numbering Notes

- **[1-5]**: Core diffusion-based methods
- **[6-9]**: Sampling-based and variational methods
- **[10-15]**: RL-based methods
- **[16]**: Unsupervised methods
- **[17-18]**: Generalist and problem reduction approaches
- **[19-21]**: Classical and hybrid methods
- **[22-23]**: ATSP-specific methods
- **[24]**: Other neural methods
- **[25-32]**: Theoretical foundations (diffusion models, optimization theory)

---

## Usage in Responses

These references are cited across:
- **Response to Weakness 1**: Covers comparisons with recent work, method positioning, and scope clarifications
- **Response to Weakness 2**: Discusses relationships with concurrent developments and alternative paradigms
- **Response to Weakness 4**: Theoretical justification for linear mixing schedule using SNR-based diffusion theory [25, 31, 32]
- **Response to Weakness 5**: Component ablation study isolating EGNN's contribution

All references correspond to entries in the manuscript's bibliography file (`iclr2026_conference.bib`).
